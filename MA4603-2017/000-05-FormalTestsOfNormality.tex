\section{Testing The Assumption of Normality}
For example, a fundamental assumption of linear models (i.e. regression models) is that the residuals (differences between observed and predicted value) are normally distributed with mean zero.


The null hypothesis of both the `Anderson-Darling' and `Shapiro-Wilk' tests is that the population is normally distributed, and the alternative hypothesis is that the data is not normally distributed.

For both tests, the null and alternative hypothesis are :\\
\qquad $H_0 : $ The data set is normally distributed.\\
\qquad $H_1 : $ The data set is \textbf{not} normally distributed.\\

\subsection{Anderson-Darling Test}
To implement the Anderson-Darling Test for Normality, one must first install the \textbf{\emph{nortest}} package.

\begin{framed}
\begin{verbatim}
library(nortest)
#Generate 100 normally distributed random numbers
NormDat = rnorm(100)
ad.test(NormDat)
\end{verbatim}
\end{framed}
\subsection{Shapiro-Wilk Test}
The Shapiro-Wilk test is directly implementable, without loading any additional packages.

\begin{framed}
\begin{verbatim}
#Generate 100 normally distributed random numbers

NormDat = rnorm(100)

shapiro.test(NormDat)
\end{verbatim}
\end{framed}
Sample output, using the randomly generated \texttt{NormDat} data set, is as follows:
\begin{verbatim}
> shapiro.test(NormDat)

        Shapiro-Wilk normality test

data:  NormDat
W = 0.9864, p-value = 0.4003
\end{verbatim}
Here, the p-value is well above the 0.05 threshold. Hence we \textbf{fail to reject} the null hypothesis, and may proceed to treat the \texttt{NormDat} data set as normally distributed.
\subsection{Graphical Procedures for Assessing Normality}
There are two useful graphical methods for determining whether a data set was normally distributed. The first is the histogram, which we have seen previously. If the histogram is reasonably bell-shaped, then the data can be assumed to be normally distributed. The relevant R command is \texttt{\textbf{hist()}}.


The second is the \textbf{\emph{quantile-quantile plot}} (or QQ-plot).
For assessing normality, we implement a qq-plot  using the \texttt{\textbf{qqnorm()}} function.

Additionally the command \texttt{\textbf{qqline()}} function adds a trendline to a normal quantile-quantile plot. If the data is normally distributed, then the points on the plot follow the trendline.

\begin{framed}
\begin{verbatim}
#Generate 100 normally distributed random numbers

NormDat = rnorm(100)

qqnorm(NormDat)
qqline(NormDat)
\end{verbatim}
\end{framed}

% Section 8 Testing Normality
\subsection{Transforming the Data}

Sometimes when we get non-normal data, we can change the scale of our data i.e. transform it to get a normal distribution. One transformation that often works for positively skewed data is the natural logarithm (ln) transformation.

In such a case, we work with the natural logarithms of the data set, rather than the data itself.


\section{Assumption of Normality}
\begin{itemize}
	\item One of the assumptions of many statistical procedures (including the t test) is that the population from which you are sampling is normally distributed. The $t-$test is said to be rather ‘robust’ in terms of this assumption, which means that reality can deviate from this assumption a fair amount without seriously affecting the validity of the analysis. 
	
	\item This is particularly true when the size of the samples is large (thanks to the Central Limit Theorem). Some deviations from normality can pose a problem for the $t-$test, specifically those that involve getting extreme scores more frequently then you would if the distribution were normal. 
	
	\item Distributions that have a greater than ‘normal’ probability of extreme scores in both directions are said to be fat tailed (because if you look at those distribution the tails look fat compared to the normal distribution). If the distribution has a fat tail on only one side it is called a skewed distribution. 
	
	\item Statistical Software Packages provides two statistical tests for deviation from normality, the 'Kolomogorov-Smirnov' test and the 'Shapiro-Wilk' test. Of the two, the Shapiro-Wilk (1965) test seems to be preferable.
	
	\item The null hypothesis of the tests is that the population is normally distributed, and the alternative hypothesis is that it is not normally distributed. 
\end{itemize}

	

\subsection{Limitations of Tests}
There are some important limitations to the usefulness of these tests.

\begin{itemize}
	\item Because it involves null hypothesis significance testing, if you reject H0 you can conclude that the population is not normally distributed, but if you don't reject H0 then you only conclude that you failed to show the population is not normally distributed. In other words, you can prove the population is not normally distributed but you can't prove it is normally distributed.
	
	\item Rejecting H0 means that the population is not normally distributed, but it doesn't tell you whether it is because it is a fat-tailed distribution, a thin-tailed distribution, a skewed distribution, or something else. As we have seen, some of these deviations from normality are much more a problem than others.
	
	\item The tests are influenced by power. If you have a small sample the test may not have enough power to detect non-normality in the population (and it is when N is small that we usually have to worry the most because of the Central Limit Theorem). If you have a very large sample the test will detect even trivial deviations from normality, those we don't really have to worry about.
\end{itemize}

